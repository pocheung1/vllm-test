{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebeb0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.exceptions import RestException\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# Constants\n",
    "BASE_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "BASE_MODEL_NAME = \"TinyLlama-1.1B-Chat-v1.0\"\n",
    "MERGED_MODEL_NAME = BASE_MODEL_NAME + \"-finetuned\"\n",
    "ADAPTER_OUTPUT_DIR = \"./adapter_weights\"\n",
    "LOG_MERGED_MODEL = True\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7197c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string() -> str:\n",
    "    return ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    "\n",
    "\n",
    "def is_model_registered(model_name: str) -> bool:\n",
    "    try:\n",
    "        client.get_registered_model(model_name)\n",
    "        return True\n",
    "    except RestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(example[\"quote\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "728320cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Retrieve the base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "if not is_model_registered(BASE_MODEL_NAME):\n",
    "    print(f\"Registering {BASE_MODEL_ID}...\")\n",
    "    mlflow.set_experiment(BASE_MODEL_NAME + \"-\" + random_string())\n",
    "    with mlflow.start_run(run_name=\"log-base-model\") as base_run:\n",
    "        model_info = mlflow.transformers.log_model(\n",
    "            transformers_model=pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer),\n",
    "            tokenizer=tokenizer,\n",
    "            artifact_path=\"base_model\",\n",
    "            input_example=\"What is the capital of France?\"\n",
    "        )\n",
    "        mlflow.register_model(model_info.model_uri, BASE_MODEL_NAME)\n",
    "        print(\"Registered base model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fdd4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter applied.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply LoRA adapter\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "print(\"LoRA adapter applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "990b46ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5242db430d9c4e6b9282d23e425171b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38719042d284d73989c556790ac1637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2619/2751584106.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset and trainer.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare dataset and trainer\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")['train'].train_test_split(test_size=0.1)\n",
    "tokenized = dataset.map(tokenize)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Prepared dataset and trainer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd518b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [565/565 01:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.267800</td>\n",
       "      <td>2.264758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "2025/07/18 10:00:42 INFO mlflow.transformers.signature: Running model prediction to infer the model output signature with a timeout of 180 seconds. You can specify a different timeout by setting the environment variable MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT.\n",
      "/opt/conda/lib/python3.10/site-packages/mlflow/transformers/signature.py:150: FutureWarning: ``mlflow.transformers.signature.generate_signature_output`` is deprecated since 2.19.0. This method will be removed in a future release. Use ``the `input_example` parameter in mlflow.transformers.log_model`` instead.\n",
      "  prediction = generate_signature_output(\n",
      "2025/07/18 10:00:52 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef0935555a44e30993f89c4c841a334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Registered model 'TinyLlama-1.1B-Chat-v1.0-finetuned' already exists. Creating a new version of this model...\n",
      "2025/07/18 10:03:13 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TinyLlama-1.1B-Chat-v1.0-finetuned, version 2\n",
      "Created version '2' of model 'TinyLlama-1.1B-Chat-v1.0-finetuned'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model registered: TinyLlama-1.1B-Chat-v1.0-finetuned.\n",
      "ðŸƒ View run adapter-finetune at: http://127.0.0.1:8768/#/experiments/8/runs/46561c95bf0d4e63b5f9a9420bb7611d\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fine-tuning\n",
    "mlflow.set_experiment(MERGED_MODEL_NAME + \"-\" + random_string())\n",
    "\n",
    "with mlflow.start_run(run_name=\"adapter-finetune\") as run:\n",
    "    mlflow.log_params({\n",
    "        \"registered_base_model\": BASE_MODEL_ID,\n",
    "        \"adapter_type\": \"LoRA\",\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"epochs\": training_args.num_train_epochs\n",
    "    })\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Log adapter weights only\n",
    "    model.save_pretrained(ADAPTER_OUTPUT_DIR)\n",
    "    mlflow.log_artifacts(ADAPTER_OUTPUT_DIR, artifact_path=\"adapters\")\n",
    "\n",
    "    # Optionally merge and register final model\n",
    "    if LOG_MERGED_MODEL:\n",
    "        merged_model = model.merge_and_unload()\n",
    "        merged_info = mlflow.transformers.log_model(\n",
    "            transformers_model=pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer),\n",
    "            tokenizer=tokenizer,\n",
    "            artifact_path=\"merged_model\",\n",
    "            input_example=\"What is the capital of France?\"\n",
    "        )\n",
    "        mlflow.register_model(merged_info.model_uri, MERGED_MODEL_NAME)\n",
    "        print(f\"Merged model registered: {MERGED_MODEL_NAME}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f9706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow-artifacts:/mlflow/6d7019af7eaf42e9a17a6a229af0c32f/artifacts\n"
     ]
    }
   ],
   "source": [
    "run = client.get_run(\"6d7019af7eaf42e9a17a6a229af0c32f\")\n",
    "print(run.info.artifact_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_registered_model(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
