{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d964c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/26 04:35:47 WARNING mlflow.transformers: The model card could not be retrieved from the hub due to Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/data/test-fe8a4e/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6'. Use `repo_type` argument if needed.\n",
      "2025/07/26 04:35:47 WARNING mlflow.transformers: Unable to find license information for this model. Please verify permissible usage for the model you are storing prior to use.\n",
      "2025/07/26 04:35:47 INFO mlflow.transformers.signature: Running model prediction to infer the model output signature with a timeout of 180 seconds. You can specify a different timeout by setting the environment variable MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT.\n",
      "/opt/conda/lib/python3.10/site-packages/mlflow/transformers/signature.py:150: FutureWarning: ``mlflow.transformers.signature.generate_signature_output`` is deprecated since 2.19.0. This method will be removed in a future release. Use ``the `input_example` parameter in mlflow.transformers.log_model`` instead.\n",
      "  prediction = generate_signature_output(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ba4c01925c472bb2c21c1b23751252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import mlflow.transformers\n",
    "\n",
    "# Input arguments\n",
    "model_dir = \"/mnt/data/test-fe8a4e/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "revision = \"fe8a4ea1ffedaf415f4da2f062534de366a451e6\"\n",
    "model_type = \"LLM\"\n",
    "\n",
    "# Load the model and tokenizer from model_dir based on model_type\n",
    "if model_type == \"LLM\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(model_dir)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "print(\"Loaded model and tokenizer\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the model\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": model, \"tokenizer\": tokenizer},\n",
    "        artifact_path=\"model\",\n",
    "        task=\"text-generation\",\n",
    "        input_example=\"Hello, how are you?\",\n",
    "        model_card=None,  # âœ… Skip HF model card fetch\n",
    "        metadata={\"hf_source\": \"local\"},  # optional, to clarify source\n",
    "    )\n",
    "    print(\"Logged model\")\n",
    "\n",
    "    # TODO Check for metadata conflicts if the registered model already exists.\n",
    "    # For example, the model_id must match the existing tag.\n",
    "\n",
    "    # Register the model\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/model\",\n",
    "        name=model_id,\n",
    "        description=f\"{model_id} registered from Hugging Face\",\n",
    "    )\n",
    "    print(f\"Registered model={model_id}, version={result.version}\")\n",
    "    \n",
    "client = MlflowClient()\n",
    "\n",
    "# Tag the registered model\n",
    "if result.version == 1:\n",
    "    client.set_registered_model_tag(name=model_id, key=\"mlflow.domino.model_id\", value=model_id)\n",
    "    client.set_registered_model_tag(name=model_id, key=\"mlflow.domino.model_type\", value=model_type)\n",
    "\n",
    "# Tag the registered model version\n",
    "client.set_model_version_tag(\n",
    "    name=model_id,\n",
    "    version=result.version,\n",
    "    key=\"mlflow.domino.model_version\",\n",
    "    value=revision,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb49990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "drwxr-xr-x. 6 root root 6144 Jul 26 02:00 models--TinyLlama--TinyLlama-1.1B-Chat-v1.0\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /mnt/data/test-fe8a4e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf9673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
