{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d964c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/26 08:44:55 WARNING mlflow.transformers: The model card could not be retrieved from the hub due to Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/data/test-fe8a4e/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6'. Use `repo_type` argument if needed.\n",
      "2025/07/26 08:44:55 WARNING mlflow.transformers: Unable to find license information for this model. Please verify permissible usage for the model you are storing prior to use.\n",
      "2025/07/26 08:44:55 INFO mlflow.transformers.signature: Running model prediction to infer the model output signature with a timeout of 180 seconds. You can specify a different timeout by setting the environment variable MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT.\n",
      "/opt/conda/lib/python3.10/site-packages/mlflow/transformers/signature.py:150: FutureWarning: ``mlflow.transformers.signature.generate_signature_output`` is deprecated since 2.19.0. This method will be removed in a future release. Use ``the `input_example` parameter in mlflow.transformers.log_model`` instead.\n",
      "  prediction = generate_signature_output(\n",
      "2025/07/26 08:45:16 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aabf8ee5334e48bd9c48a7e8e6c3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'TinyLlama_TinyLlama-1.1B-Chat-v1.0' already exists. Creating a new version of this model...\n",
      "2025/07/26 08:47:52 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TinyLlama_TinyLlama-1.1B-Chat-v1.0, version 2\n",
      "Created version '2' of model 'TinyLlama_TinyLlama-1.1B-Chat-v1.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model=TinyLlama_TinyLlama-1.1B-Chat-v1.0, version=2\n",
      "🏃 View run whimsical-colt-607 at: http://127.0.0.1:8768/#/experiments/17/runs/846fa217aca349e780d00fcfb1b2992f\n",
      "🧪 View experiment at: http://127.0.0.1:8768/#/experiments/17\n",
      "Tagged mlflow.domino.model_id={model_id}\n",
      "Tagged mlflow.domino.model_type=LLM\n",
      "Tagged mlflow.domino.model_version={revision}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'description' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Update description\u001b[39;00m\n\u001b[1;32m     62\u001b[0m client\u001b[38;5;241m.\u001b[39mupdate_registered_model(\n\u001b[1;32m     63\u001b[0m     name\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m     64\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from Hugging Face\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated registered model description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdescription\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'description' is not defined"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import mlflow.transformers\n",
    "\n",
    "\n",
    "# Input arguments\n",
    "model_dir = \"/mnt/data/test-fe8a4e/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6\"\n",
    "model_id = \"TinyLlama_TinyLlama-1.1B-Chat-v1.0\"\n",
    "revision = \"fe8a4ea1ffedaf415f4da2f062534de366a451e6\"\n",
    "model_type = \"LLM\"\n",
    "\n",
    "\n",
    "# Load the model and tokenizer from model_dir based on model_type\n",
    "if model_type == \"LLM\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(model_dir)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "print(\"Loaded model and tokenizer\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the model\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": model, \"tokenizer\": tokenizer},\n",
    "        artifact_path=\"model\",\n",
    "        task=\"text-generation\",\n",
    "        input_example=\"Hello, how are you?\",\n",
    "    )\n",
    "    print(\"Logged model\")\n",
    "\n",
    "    # TODO Check for metadata conflicts if the registered model already exists.\n",
    "    # For example, the model_id must match the existing tag.\n",
    "\n",
    "    # Register the model\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/model\",\n",
    "        name=model_id,\n",
    "    )\n",
    "    print(f\"Registered model={model_id}, version={result.version}\")\n",
    "    \n",
    "client = MlflowClient()\n",
    "\n",
    "# Tag model ID\n",
    "client.set_registered_model_tag(name=model_id, key=\"mlflow.domino.model_id\", value=model_id)\n",
    "\n",
    "# Tag model type\n",
    "client.set_registered_model_tag(name=model_id, key=\"mlflow.domino.model_type\", value=model_type)\n",
    "    \n",
    "# Tag model version\n",
    "client.set_model_version_tag(\n",
    "    name=model_id,\n",
    "    version=result.version,\n",
    "    key=\"mlflow.domino.model_version\",\n",
    "    value=revision,\n",
    ")\n",
    "\n",
    "# Update description\n",
    "client.update_registered_model(\n",
    "    name=model_id,\n",
    "    description=f\"{model_id} from Hugging Face\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf9673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
