{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d964c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/26 05:47:06 WARNING mlflow.transformers: The model card could not be retrieved from the hub due to Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/data/test-fe8a4e/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6'. Use `repo_type` argument if needed.\n",
      "2025/07/26 05:47:06 WARNING mlflow.transformers: Unable to find license information for this model. Please verify permissible usage for the model you are storing prior to use.\n",
      "2025/07/26 05:47:06 INFO mlflow.transformers.signature: Running model prediction to infer the model output signature with a timeout of 180 seconds. You can specify a different timeout by setting the environment variable MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT.\n",
      "/opt/conda/lib/python3.10/site-packages/mlflow/transformers/signature.py:150: FutureWarning: ``mlflow.transformers.signature.generate_signature_output`` is deprecated since 2.19.0. This method will be removed in a future release. Use ``the `input_example` parameter in mlflow.transformers.log_model`` instead.\n",
      "  prediction = generate_signature_output(\n",
      "2025/07/26 05:48:04 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846e99c352ec408db91763532e929b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'TinyLlama--TinyLlama-1.1B-Chat-v1.0'.\n",
      "2025/07/26 05:50:57 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TinyLlama--TinyLlama-1.1B-Chat-v1.0, version 1\n",
      "Created version '1' of model 'TinyLlama--TinyLlama-1.1B-Chat-v1.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model=TinyLlama--TinyLlama-1.1B-Chat-v1.0, version=1\n",
      "🏃 View run mysterious-fowl-720 at: http://127.0.0.1:8768/#/experiments/17/runs/8b2dc22aa0fa47b08f6ee324501036fd\n",
      "🧪 View experiment at: http://127.0.0.1:8768/#/experiments/17\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import mlflow.transformers\n",
    "\n",
    "\n",
    "# Input arguments\n",
    "model_dir = \"/mnt/data/test-fe8a4e/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6\"\n",
    "model_id = \"TinyLlama__TinyLlama-1.1B-Chat-v1.0\"\n",
    "revision = \"fe8a4ea1ffedaf415f4da2f062534de366a451e6\"\n",
    "model_type = \"LLM\"\n",
    "\n",
    "\n",
    "# Load the model and tokenizer from model_dir based on model_type\n",
    "if model_type == \"LLM\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(model_dir)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "print(\"Loaded model and tokenizer\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the model\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": model, \"tokenizer\": tokenizer},\n",
    "        artifact_path=\"model\",\n",
    "        task=\"text-generation\",\n",
    "        input_example=\"Hello, how are you?\",\n",
    "    )\n",
    "    print(\"Logged model\")\n",
    "\n",
    "    # TODO Check for metadata conflicts if the registered model already exists.\n",
    "    # For example, the model_id must match the existing tag.\n",
    "\n",
    "    # Register the model\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/model\",\n",
    "        name=model_id,\n",
    "    )\n",
    "    print(f\"Registered model={model_id}, version={result.version}\")\n",
    "    \n",
    "client = MlflowClient()\n",
    "\n",
    "# First-time registration\n",
    "if result.version == \"1\":\n",
    "    # Update description\n",
    "    client.update_registered_model(\n",
    "        name=model_id,\n",
    "        description=f\"{model_id} @ {model_version} from Hugging Face\"\n",
    "    )\n",
    "    print(f\"Updated registered model description: {description}\")\n",
    "\n",
    "    # Tag model ID\n",
    "    client.set_registered_model_tag(name=model_id, key=\"mlflow.domino.model_id\", value=model_id)\n",
    "    print(\"Tagged mlflow.domino.model_id={model_id}\")\n",
    "\n",
    "# Tag model type\n",
    "client.set_registered_model_tag(name=model_id, key=\"mlflow.domino.model_type\", value=model_type)\n",
    "print(f\"Tagged mlflow.domino.model_type={model_type}\")\n",
    "    \n",
    "# Tag model version\n",
    "client.set_model_version_tag(\n",
    "    name=model_id,\n",
    "    version=result.version,\n",
    "    key=\"mlflow.domino.model_version\",\n",
    "    value=revision,\n",
    ")\n",
    "print(\"Tagged mlflow.domino.model_version={model_version}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf9673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
