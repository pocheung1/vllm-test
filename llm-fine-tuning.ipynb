{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebeb0918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/mlflow', creation_time=1752871141895, experiment_id='12', last_update_time=1752871141895, lifecycle_stage='active', name='TinyLlama-fine-tuning', tags={'mlflow.domino.dataset_info': '68788688a685c05b1700ea8c-68788688a685c05b1700ea8b',\n",
       " 'mlflow.domino.environment_id': '687895e6a685c05b1700eab5',\n",
       " 'mlflow.domino.environment_revision_id': '6879b872da87d040dba4627b',\n",
       " 'mlflow.domino.hardware_tier': 'gpu-small-k8s',\n",
       " 'mlflow.domino.project_id': '68788685a685c05b1700ea86',\n",
       " 'mlflow.domino.project_name': 'LLM',\n",
       " 'mlflow.domino.run_id': '687a7ff3b2eee2648e0ace71',\n",
       " 'mlflow.domino.run_number': '17',\n",
       " 'mlflow.domino.user': 'integration-test',\n",
       " 'mlflow.domino.user_id': '68788292fc17b3228539ea3e',\n",
       " 'mlflow.source.type': 'NOTEBOOK',\n",
       " 'mlflow.user': 'integration-test'}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.exceptions import RestException\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "BASE_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "BASE_MODEL_NAME = \"TinyLlama-1.1B-Chat-v1.0\"\n",
    "ADAPTER_MODEL_NAME = BASE_MODEL_NAME + \"-adapter\"\n",
    "MERGED_MODEL_NAME = BASE_MODEL_NAME + \"-merged\"\n",
    "\n",
    "ADAPTER_OUTPUT_DIR = \"adapter\"\n",
    "ADAPTER_ARTIFACT_PATH = \"adapter\"\n",
    "\n",
    "REGISTER_ADAPTER_MODEL = True\n",
    "REGISTER_MERGED_MODEL = False\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "mlflow.set_experiment(\"TinyLlama-fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7197c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_model_registered(model_name: str) -> bool:\n",
    "    try:\n",
    "        client.get_registered_model(model_name)\n",
    "        return True\n",
    "    except RestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(example[\"quote\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728320cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a0e3acea7142bf9b4cac82b5443cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211faed7d693412ca2cf1f42bb87fe1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa846c261084aeeb0aa448ff6f557fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6c3823ae014bb9b5cae283ced71612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dfd56d465945fa915dea8ed2f2258b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80485379ef124d5fa845e3443b353e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bc833391794728add38267a6fc31e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded base model and tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Retrieve the base model and tokenizer\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "print(\"Downloaded base model and tokenizer\")\n",
    "\n",
    "if not is_model_registered(BASE_MODEL_NAME):\n",
    "    print(f\"Registering {BASE_MODEL_ID}...\")\n",
    "\n",
    "    with mlflow.start_run(run_name=\"log-base-model\") as base_run:\n",
    "        model_info = mlflow.transformers.log_model(\n",
    "            transformers_model=pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer),\n",
    "            tokenizer=tokenizer,\n",
    "            artifact_path=\"base_model\",\n",
    "            input_example=\"What's the capital of France?\"\n",
    "        )\n",
    "        mlflow.register_model(model_info.model_uri, BASE_MODEL_NAME)\n",
    "        print(f\"Registered base model: {BASE_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fdd4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter applied\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply LoRA adapter\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "print(\"LoRA adapter applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990b46ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ce94596f8941fb8ac6c82f0905436e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e6f12c00b0428581218fee4957b54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180c86683d6b479c9b4e05e921cae246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805d8fcc303d4991a5392075937b1e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a14c280538449ab2e659f2be19e442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11648/2739314731.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset and trainer\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare dataset and trainer\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")['train'].train_test_split(test_size=0.1)\n",
    "tokenized = dataset.map(tokenize)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Prepared dataset and trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd518b34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [565/565 01:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.181300</td>\n",
       "      <td>2.290818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/19 03:25:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TinyLlama-1.1B-Chat-v1.0-adapter, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged adapter weights\n",
      "Registered adapter weights\n",
      "üèÉ View run adapter-finetune at: http://127.0.0.1:8768/#/experiments/12/runs/44a7aa84144749efb21ec713f4b8fefc\n",
      "üß™ View experiment at: http://127.0.0.1:8768/#/experiments/12\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fine-tuning\n",
    "\n",
    "with mlflow.start_run(run_name=\"adapter-finetune\") as run:\n",
    "    mlflow.log_params({\n",
    "        \"registered_base_model\": BASE_MODEL_ID,\n",
    "        \"adapter_type\": \"LoRA\",\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"epochs\": training_args.num_train_epochs\n",
    "    })\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Log adapter weights only\n",
    "    peft_model.save_pretrained(ADAPTER_OUTPUT_DIR)\n",
    "    mlflow.log_artifacts(local_dir=ADAPTER_OUTPUT_DIR, artifact_path=ADAPTER_ARTIFACT_PATH)\n",
    "    print(\"Logged adapter weights\")\n",
    "    \n",
    "    if REGISTER_ADAPTER_MODEL:\n",
    "        source = f\"runs:/{run.info.run_id}/{ADAPTER_ARTIFACT_PATH}\"\n",
    "        client.create_registered_model(ADAPTER_MODEL_NAME)\n",
    "        client.create_model_version(ADAPTER_MODEL_NAME, source, run.info.run_id)\n",
    "        print(\"Registered adapter weights\")\n",
    "\n",
    "    # Optionally merge and register final model\n",
    "    if REGISTER_MERGED_MODEL:\n",
    "        merged_model = peft_model.merge_and_unload()\n",
    "        merged_info = mlflow.transformers.log_model(\n",
    "            transformers_model=pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer),\n",
    "            tokenizer=tokenizer,\n",
    "            artifact_path=\"merged_model\",\n",
    "            input_example=\"What's the capital of France?\"\n",
    "        )\n",
    "        mlflow.register_model(merged_info.model_uri, MERGED_MODEL_NAME)\n",
    "        print(f\"Registered merged model: {MERGED_MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - delete registered models\n",
    "\n",
    "for model_name in [BASE_MODEL_NAME, ADAPTER_MODEL_NAME, MERGED_MODEL_NAME]:\n",
    "    try:\n",
    "        client.delete_registered_model(name=model_name)\n",
    "        print(f\"Deleted registered model: {model_name}\")\n",
    "    except RestException as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef501a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - delete experiment by id\n",
    "\n",
    "client.delete_experiment(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - delete experiment run by id\n",
    "\n",
    "client.delete_run(\"ee2880594f034b988c41db2dfbbd8b44\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934c8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
