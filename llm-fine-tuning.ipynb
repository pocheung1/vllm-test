{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebeb0918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/mlflow', creation_time=1752871141895, experiment_id='12', last_update_time=1752871141895, lifecycle_stage='active', name='TinyLlama-fine-tuning', tags={'mlflow.domino.dataset_info': '68788688a685c05b1700ea8c-68788688a685c05b1700ea8b',\n",
       " 'mlflow.domino.environment_id': '687895e6a685c05b1700eab5',\n",
       " 'mlflow.domino.environment_revision_id': '6879b872da87d040dba4627b',\n",
       " 'mlflow.domino.hardware_tier': 'gpu-small-k8s',\n",
       " 'mlflow.domino.project_id': '68788685a685c05b1700ea86',\n",
       " 'mlflow.domino.project_name': 'LLM',\n",
       " 'mlflow.domino.run_id': '687a7ff3b2eee2648e0ace71',\n",
       " 'mlflow.domino.run_number': '17',\n",
       " 'mlflow.domino.user': 'integration-test',\n",
       " 'mlflow.domino.user_id': '68788292fc17b3228539ea3e',\n",
       " 'mlflow.source.type': 'NOTEBOOK',\n",
       " 'mlflow.user': 'integration-test'}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.exceptions import RestException\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "BASE_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "BASE_MODEL_NAME = \"TinyLlama-1.1B-Chat-v1.0\"\n",
    "MERGED_MODEL_NAME = BASE_MODEL_NAME + \"-finetuned\"\n",
    "ADAPTER_OUTPUT_DIR = \"./adapter_weights\"\n",
    "LOG_MERGED_MODEL = True\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "mlflow.set_experiment(\"TinyLlama-fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7197c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_model_registered(model_name: str) -> bool:\n",
    "    try:\n",
    "        client.get_registered_model(model_name)\n",
    "        return True\n",
    "    except RestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(example[\"quote\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728320cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded base model and tokenizer\n",
      "Registering TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "2025/07/18 20:41:04 INFO mlflow.transformers.signature: Running model prediction to infer the model output signature with a timeout of 180 seconds. You can specify a different timeout by setting the environment variable MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT.\n",
      "/opt/conda/lib/python3.10/site-packages/mlflow/transformers/signature.py:150: FutureWarning: ``mlflow.transformers.signature.generate_signature_output`` is deprecated since 2.19.0. This method will be removed in a future release. Use ``the `input_example` parameter in mlflow.transformers.log_model`` instead.\n",
      "  prediction = generate_signature_output(\n",
      "2025/07/18 20:41:16 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ec169c27ac41bbb3157924c8e34ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Successfully registered model 'TinyLlama-1.1B-Chat-v1.0'.\n",
      "2025/07/18 20:44:21 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TinyLlama-1.1B-Chat-v1.0, version 1\n",
      "Created version '1' of model 'TinyLlama-1.1B-Chat-v1.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered base model: TinyLlama-1.1B-Chat-v1.0\n",
      "ðŸƒ View run log-base-model at: http://127.0.0.1:8768/#/experiments/12/runs/21e1e80f342a4fc5988c6da29b36971f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/12\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Retrieve the base model and tokenizer\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "print(\"Downloaded base model and tokenizer\")\n",
    "\n",
    "if not is_model_registered(BASE_MODEL_NAME):\n",
    "    print(f\"Registering {BASE_MODEL_ID}...\")\n",
    "\n",
    "    with mlflow.start_run(run_name=\"log-base-model\") as base_run:\n",
    "        model_info = mlflow.transformers.log_model(\n",
    "            transformers_model=pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer),\n",
    "            tokenizer=tokenizer,\n",
    "            artifact_path=\"base_model\",\n",
    "            input_example=\"What's the capital of France?\"\n",
    "        )\n",
    "        mlflow.register_model(model_info.model_uri, BASE_MODEL_NAME)\n",
    "        print(f\"Registered base model: {BASE_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fdd4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter applied\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply LoRA adapter\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "print(\"LoRA adapter applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990b46ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6596873487d5437990c4cb570d91f244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7e9f9fc1d543c790eb30e5a3976b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12611/510542893.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset and trainer\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare dataset and trainer\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")['train'].train_test_split(test_size=0.1)\n",
    "tokenized = dataset.map(tokenize)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Prepared dataset and trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd518b34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [565/565 01:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.255300</td>\n",
       "      <td>2.350652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged adapter weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/18 20:50:40 INFO mlflow.transformers.signature: Running model prediction to infer the model output signature with a timeout of 180 seconds. You can specify a different timeout by setting the environment variable MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT.\n",
      "/opt/conda/lib/python3.10/site-packages/mlflow/transformers/signature.py:150: FutureWarning: ``mlflow.transformers.signature.generate_signature_output`` is deprecated since 2.19.0. This method will be removed in a future release. Use ``the `input_example` parameter in mlflow.transformers.log_model`` instead.\n",
      "  prediction = generate_signature_output(\n",
      "2025/07/18 20:50:51 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4f2ce785cd4f64973ad5ae117eea7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Successfully registered model 'TinyLlama-1.1B-Chat-v1.0-finetuned'.\n",
      "2025/07/18 20:53:10 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TinyLlama-1.1B-Chat-v1.0-finetuned, version 1\n",
      "Created version '1' of model 'TinyLlama-1.1B-Chat-v1.0-finetuned'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered merged model: TinyLlama-1.1B-Chat-v1.0-finetuned\n",
      "ðŸƒ View run adapter-finetune at: http://127.0.0.1:8768/#/experiments/12/runs/f44274eed8fb41119dc390dfb0c0bfd1\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/12\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fine-tuning\n",
    "\n",
    "with mlflow.start_run(run_name=\"adapter-finetune\") as run:\n",
    "    mlflow.log_params({\n",
    "        \"registered_base_model\": BASE_MODEL_ID,\n",
    "        \"adapter_type\": \"LoRA\",\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"epochs\": training_args.num_train_epochs\n",
    "    })\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Log adapter weights only\n",
    "    model.save_pretrained(ADAPTER_OUTPUT_DIR)\n",
    "    mlflow.log_artifacts(ADAPTER_OUTPUT_DIR, artifact_path=\"adapters\")\n",
    "    print(\"Logged adapter weights\")\n",
    "\n",
    "    # Optionally merge and register final model\n",
    "    if LOG_MERGED_MODEL:\n",
    "        merged_model = model.merge_and_unload()\n",
    "        merged_info = mlflow.transformers.log_model(\n",
    "            transformers_model=pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer),\n",
    "            tokenizer=tokenizer,\n",
    "            artifact_path=\"merged_model\",\n",
    "            input_example=\"What's the capital of France?\"\n",
    "        )\n",
    "        mlflow.register_model(merged_info.model_uri, MERGED_MODEL_NAME)\n",
    "        print(f\"Registered merged model: {MERGED_MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3a25e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESOURCE_DOES_NOT_EXIST: Registered Model with name=TinyLlama-1.1B-Chat-v1.0 not found\n",
      "RESOURCE_DOES_NOT_EXIST: Registered Model with name=TinyLlama-1.1B-Chat-v1.0-finetuned not found\n"
     ]
    }
   ],
   "source": [
    "# Clean up - delete registered models\n",
    "\n",
    "for model_name in [BASE_MODEL_NAME, MERGED_MODEL_NAME]:\n",
    "    try:\n",
    "        client.delete_registered_model(name=model_name)\n",
    "        print(f\"Deleted registered model: {model_name}\")\n",
    "    except RestException as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef501a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - delete experiment by id\n",
    "\n",
    "client.delete_experiment(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a038ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - delete experiment run by id\n",
    "\n",
    "client.delete_run(\"ee2880594f034b988c41db2dfbbd8b44\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934c8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
